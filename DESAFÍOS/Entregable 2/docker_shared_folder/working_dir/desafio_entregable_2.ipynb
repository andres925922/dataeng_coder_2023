{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a0def9-6128-4135-87fd-6a7ce625ae05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install requests\n",
    "# %pip install pandas\n",
    "# %pip install urllib\n",
    "\n",
    "# %pip install python-dotenv\n",
    "# %pip install python-decouple\n",
    "\n",
    "# %pip install psycopg2-binary\n",
    "# %pip install sqlalchemy\n",
    "\n",
    "# %pip install pyspark[all]\n",
    "\n",
    "# libary included to solve not module pyspark found\n",
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fcaf4a46-f41a-45a2-a523-69a3d486fab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# datetime para manejo de fechas\n",
    "from datetime import datetime\n",
    "\n",
    "# DB connector and libraries\n",
    "import psycopg2\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "# Pyspark utils\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import to_date, col, from_unixtime\n",
    "from pyspark.sql.types import ( \n",
    "    FloatType, \n",
    "    LongType,\n",
    "    IntegerType, \n",
    "    StructType, \n",
    "    BooleanType,\n",
    "    StructField,\n",
    "    DateType \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ee936-4cfc-43d7-9202-fa34e5acf920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19dccf48-d7a9-48aa-9816-6dd843840e56",
   "metadata": {},
   "source": [
    "## Iniciar la sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8487e19e-d174-4c89-924c-4bb288a53202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Spark_Session:\n",
    "    \n",
    "    _driver_path: str = \"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "#         Define environment variables\n",
    "        self._driver_path = os.getcwd() + \"/driver_jdbc/postgresql-42.2.27.jre7.jar\"\n",
    "\n",
    "        os.environ['PYSPARK_SUBMIT_ARGS'] = f'--driver-class-path {self._driver_path} --jars {self._driver_path} pyspark-shell'\n",
    "        os.environ['SPARK_CLASSPATH'] = self._driver_path\n",
    "        # Start spark session\n",
    "        self._start_spark_session()\n",
    "        \n",
    "    def _start_spark_session(self):\n",
    "        self._spark: SparkSession = SparkSession.builder \\\n",
    "            .master(\"local\") \\\n",
    "            .appName(\"Conexion entre Pyspark y Redshift\") \\\n",
    "            .config(\"spark.jars\", self._driver_path) \\\n",
    "            .config(\"spark.executor.extraClassPath\", self._driver_path) \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    def return_session(self):\n",
    "        \"\"\"\n",
    "        Función que retorna la sessión de spark creada al instanciar la clase.\n",
    "        \"\"\"\n",
    "        return self._spark\n",
    "            \n",
    "spark = Spark_Session()\n",
    "spark_session = spark.return_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78f6bd73-6559-4299-880e-3802c2c8d41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url_constructor(url: str, **kargs: dict[any]) -> str:\n",
    "\n",
    "    return f\"{url}?{urllib.parse.urlencode(kargs)}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bb46ef-2de4-49cf-a6c5-911514e27521",
   "metadata": {},
   "source": [
    "## Obtenemos la información desde la url y creamos el df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "28ed6785-22f4-4a82-895e-472fcde8e4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vamos a consultar la información historica diaria que nos emite Binance sin limites inicio y fin\n",
    "\n",
    "data = requests.get( \\\n",
    "                url_constructor(\"https://api.binance.com/api/v3/klines\", symbol=\"BTCUSDT\", interval=\"1d\")\n",
    "        ).json()\n",
    "\n",
    "# Headers obtenidos desde la documentación de binance\n",
    "# schema = StructType([ \\\n",
    "#     StructField(\"Open time\",LongType(),True), \\\n",
    "#     StructField(\"Open\",FloatType(),True), \\\n",
    "#     StructField(\"High\",FloatType(),True), \\\n",
    "#     StructField(\"Low\", FloatType(), True), \\\n",
    "#     StructField(\"Close\", FloatType(), True), \\\n",
    "#     StructField(\"Volume\", IntegerType(), True), \\\n",
    "#     StructField(\"Close time\", LongType(), True), \\\n",
    "#     StructField(\"Quote asset volume\", FloatType(), True), \\\n",
    "#     StructField(\"Number of trades\", IntegerType(), True), \\\n",
    "#     StructField(\"Taker buy base asset volume\", FloatType(), True), \\\n",
    "#     StructField(\"Taker buy quote asset volume\", FloatType(), True), \\\n",
    "#     StructField(\"Ignore\", BooleanType(), True) \\\n",
    "# ])\n",
    "\n",
    "headers = (\"Open_time\",\n",
    "           \"Open\",\n",
    "           \"High\", \n",
    "           \"Low\", \n",
    "           \"Close\", \n",
    "           \"Volume\", \n",
    "           \"Close_time\", \n",
    "           \"Quote asset volume\", \n",
    "           \"Trades_qty\",\n",
    "           \"Taker buy base asset volume\",\n",
    "           \"Taker buy quote asset volume\", \n",
    "           \"Ignore\"\n",
    "        )\n",
    "\n",
    "# Se crea el dataframe utilizando la session de spark generada anteriormente\n",
    "df = spark_session \\\n",
    "        .createDataFrame(data, [x for x in headers])\n",
    "# df = spark_session \\\n",
    "#         .createDataFrame(data, schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ef30062e-8083-4eb3-8b6b-f04f9a08502d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parseamos los datos\n",
    "df = df.withColumn(\"Open\", df[\"Open\"].cast(FloatType()))\n",
    "df = df.withColumn(\"High\", df[\"High\"].cast(FloatType()))\n",
    "df = df.withColumn(\"Low\", df[\"Low\"].cast(FloatType()))\n",
    "df = df.withColumn(\"Close\", df[\"Close\"].cast(FloatType()))\n",
    "df = df.withColumn(\"Volume\", df[\"Volume\"].cast(FloatType()))\n",
    "# df = df.withColumn(\"Quote asset volume\", df[\"Quote asset volume\"].cast(FloatType()))\n",
    "df = df.withColumn(\"Trades_qty\", df[\"Trades_qty\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"Taker buy base asset volume\", df[\"Taker buy base asset volume\"].cast(FloatType()))\n",
    "# df = df.withColumn(\"Taker buy quote asset volume\", df[\"Taker buy quote asset volume\"].cast(FloatType()))\n",
    "# df = df.withColumn(\"Ignore\", df[\"Ignore\"].cast(BooleanType()))\n",
    "\n",
    "# Eliminamos columnas que no vamos a necesitar\n",
    "df = df.drop(\"Quote asset volume\",\"Taker buy base asset volume\",\"Taker buy quote asset volume\",\"Ignore\")\n",
    "\n",
    "# Manejo de fechas\n",
    "# # Los timestamps estan en milisegundos por lo que deben pasarse a segundos\n",
    "# # Luego los parseamos con la función from_unixtime\n",
    "df = df.withColumn(\"Open_time\", \n",
    "                   from_unixtime((df[\"Open_time\"]/1000)) \\\n",
    "                   .cast(DateType()) \\\n",
    "                   )\n",
    "df = df.withColumn(\"Close_time\", \n",
    "                   from_unixtime((df[\"Close_time\"]/1000)) \\\n",
    "                   .cast(DateType()) \\\n",
    "                    )\n",
    "\n",
    "# Creamos la columna de promedio entre los valores high and close\n",
    "df = df.withColumn(\"Daily_Avg\", (df.High + df.Low)/2)\n",
    "df = df.withColumn(\"Open_Close_Gap\", (df.Open + df.Close)/2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11af018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "|summary|              Open|             High|              Low|             Close|           Volume|      Trades_qty|        Daily_Avg|    Open_Close_Gap|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "|  count|               500|              500|              500|               500|              500|             500|              500|               500|\n",
      "|   mean|26109.764966796876|26674.98291015625|25515.16741015625|26085.773681640625|163836.8904316406|     3801760.088| 26095.0751953125|26097.769298828123|\n",
      "| stddev| 7991.312222187446|8185.175579883869|7776.166364369698| 7960.330834745574|135864.3127664517|3076266.40468566|7972.068081849734|7963.7993469477315|\n",
      "|    min|          15781.29|          16315.0|          15476.0|          15781.29|        14434.547|          415694|          15897.5|   16004.115234375|\n",
      "|    max|          47434.79|         48189.84|         46950.85|           47434.8|         760705.4|        15223589|   47523.66015625|     47278.5078125|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimos metricas del df\n",
    "df.describe().show()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f3b78a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Open_time: date]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtramos aquellos valores donde la fechas sean nulas\n",
    "df = df.filter(df['Open_time'].isNotNull() & df['Close_time'].isNotNull())\n",
    "\n",
    "# Eliminamos duplicados con la fecha open_time\n",
    "df.select(col('Open_time')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d68d5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open_time: date (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Close_time: date (nullable = true)\n",
      " |-- Trades_qty: integer (nullable = true)\n",
      " |-- Daily_Avg: double (nullable = true)\n",
      " |-- Open_Close_Gap: double (nullable = true)\n",
      "\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "|summary|              Open|             High|              Low|             Close|           Volume|      Trades_qty|        Daily_Avg|    Open_Close_Gap|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "|  count|               500|              500|              500|               500|              500|             500|              500|               500|\n",
      "|   mean|26109.764966796876|26674.98291015625|25515.16741015625|26085.773681640625|163836.8904316406|     3801760.088| 26095.0751953125|26097.769298828123|\n",
      "| stddev| 7991.312222187446|8185.175579883869|7776.166364369698| 7960.330834745574|135864.3127664517|3076266.40468566|7972.068081849734|7963.7993469477315|\n",
      "|    min|          15781.29|          16315.0|          15476.0|          15781.29|        14434.547|          415694|          15897.5|   16004.115234375|\n",
      "|    max|          47434.79|         48189.84|         46950.85|           47434.8|         760705.4|        15223589|   47523.66015625|     47278.5078125|\n",
      "+-------+------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+\n",
      "\n",
      "+----------+--------+--------+--------+--------+---------+----------+----------+--------------+--------------+\n",
      "| Open_time|    Open|    High|     Low|   Close|   Volume|Close_time|Trades_qty|     Daily_Avg|Open_Close_Gap|\n",
      "+----------+--------+--------+--------+--------+---------+----------+----------+--------------+--------------+\n",
      "|2022-02-12|42373.73|43079.49|41688.88|42217.87|26556.857|2022-02-12|    830322|42384.18359375|42295.80078125|\n",
      "|2022-02-13|42217.87| 42760.0| 41870.0|42053.66|17732.082|2022-02-13|    624898|       42315.0|  42135.765625|\n",
      "|2022-02-14|42053.65| 42842.4|41550.56|42535.94| 34010.13|2022-02-14|    867622| 42196.4765625|  42294.796875|\n",
      "|2022-02-15|42535.94| 44751.4|42427.03|44544.86|38095.195|2022-02-15|    990694|43589.21484375| 43540.3984375|\n",
      "|2022-02-16|44544.85|44549.97| 43307.0|43873.56|28471.873|2022-02-16|    823745|  43928.484375|  44209.203125|\n",
      "|2022-02-17|43873.56|44164.71|40073.21| 40515.7|47245.996|2022-02-17|   1067658| 42118.9609375|42194.62890625|\n",
      "|2022-02-18|40515.71|40959.88| 39450.0|39974.44| 43845.92|2022-02-18|   1025882|    40204.9375|  40245.078125|\n",
      "|2022-02-19|39974.45|40444.32|39639.03|40079.17|18042.055|2022-02-19|    586653|40041.67578125|    40026.8125|\n",
      "|2022-02-20|40079.17|40125.44| 38000.0|38386.89| 33439.29|2022-02-20|    811235|   39062.71875|   39233.03125|\n",
      "|2022-02-21|38386.89|39494.35| 36800.0|37008.16|62347.684|2022-02-21|   1352290|38147.17578125| 37697.5234375|\n",
      "|2022-02-22|37008.16| 38429.0| 36350.0|38230.33|53785.945|2022-02-22|   1212527|       37389.5| 37619.2421875|\n",
      "|2022-02-23|38230.33|39249.93|37036.79|37250.01| 43560.73|2022-02-23|   1030872|  38143.359375|  37740.171875|\n",
      "|2022-02-24|37250.02| 39843.0|34322.28|38327.21| 120476.3|2022-02-24|   2622780|  37082.640625| 37788.6171875|\n",
      "|2022-02-25|38328.68|39683.53|38014.37|39219.17| 56574.57|2022-02-25|   1408558|  38848.953125|38773.92578125|\n",
      "|2022-02-26|39219.16|40348.45|38573.18|39116.72|29361.256|2022-02-26|    924257|    39460.8125|    39167.9375|\n",
      "|2022-02-27|39116.73| 39855.7| 37000.0|37699.07|46229.445|2022-02-27|   1265753| 38427.8515625| 38407.8984375|\n",
      "|2022-02-28|37699.08|44225.84|37450.17| 43160.0| 73945.64|2022-02-28|   1931087| 40838.0078125| 40429.5390625|\n",
      "|2022-03-01| 43160.0| 44949.0|42809.98| 44421.2|61743.098|2022-03-01|   1866871| 43879.4921875| 43790.6015625|\n",
      "|2022-03-02| 44421.2| 45400.0|43334.09|43892.98|57782.652|2022-03-02|   1653131|  44367.046875|44157.08984375|\n",
      "|2022-03-03|43892.99|44101.12|41832.28| 42454.0| 50940.61|2022-03-03|   1289320|  42966.703125| 43173.4921875|\n",
      "+----------+--------+--------+--------+--------+---------+----------+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.describe().show()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e74f03f-a3c2-4c31-9ba6-b6e174a19f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/12/2022 00:00:00'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(1644624000000/1000).strftime(\"%m/%d/%Y %H:%M:%S\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afadc7f1",
   "metadata": {},
   "source": [
    "## Creamos las funcionalidades de la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e1612ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL():\n",
    "\n",
    "    _table_name = \"bitcoin_daily_prices\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Configuramos redshift importando la data desde constantes de entorno\n",
    "        self._HOST = os.environ['REDSHIFT_HOST']\n",
    "        self._PORT = os.environ['REDSHIFT_PORT']\n",
    "        self._DB = os.environ['REDSHIFT_DB']\n",
    "        self._USER = os.environ[\"REDSHIFT_USER\"]\n",
    "        self._DB_NAME = os.environ[\"REDSHIFT_DB_NAME\"]\n",
    "        self._SCHEMA = os.environ[\"REDSHIFT_SCHEMA\"]\n",
    "        self._PASSWORD = os.environ[\"REDSHIFT_PASSWORD\"]\n",
    "        self._DRIVER = \"io.github.spark_redshift_community.spark.redshift\"\n",
    "        self._URL = f\"jdbc:redshift:// \\\n",
    "        {self._HOST}:{self._PORT}/{self._DB}?user= \\\n",
    "        {self._USER}&password={self._PASSWORD}\"\n",
    "\n",
    "        # Iniciamos la conexión\n",
    "        self._create_engine()\n",
    "        # Creamos la tabla en el warehouse\n",
    "        self._create_table()\n",
    "\n",
    "    def _create_table(self):\n",
    "        cursor = self._conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(f\"\"\"\n",
    "            create table if not exists \n",
    "                {self._SCHEMA}.{self._table_name} (\n",
    "                OPEN_TIME DATETIME distkey,\n",
    "                OPEN DECIMAL(10,2) NOT NULL,\n",
    "                HIGH DECIMAL(10,2) NOT NULL,\n",
    "                LOW DECIMAL(10,2) NOT NULL,\n",
    "                CLOSE DECIMAL(10,2) NOT NULL,\n",
    "                VOLUME DECIMAL(10,2) NOT NULL,\n",
    "                CLOSE_TIME DATETIME,\n",
    "                TADES_QTY INT NOT NULL,\n",
    "                Daily_Avg DECIMAL(10,2) NOT NULL,\n",
    "                Open_Close_Gap DECIMAL(10,2) NOT NULL\n",
    "                \n",
    "            ) sortkey(OPEN_TIME);\n",
    "            \"\"\")\n",
    "            self._conn.commit()\n",
    "            print(\"Ejecución correcta\")\n",
    "        except: raise Exception(\"Ha ocurrido un error\")\n",
    "            # No es el mejor manejo de errores pero para esto esta bien.\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "    def _create_engine(self) -> None:\n",
    "        # Creamos la conexión a la db\n",
    "        try:\n",
    "            self._conn = psycopg2.connect(\n",
    "                host = self._HOST,\n",
    "                port = self._PORT,\n",
    "                dbname = self._DB_NAME,\n",
    "                user = self._USER,\n",
    "                password = self._PASSWORD\n",
    "            )\n",
    "        except: raise Exception(\"Ha ocurrido un error al tartar de conectarse a la db\")\n",
    "\n",
    "    \n",
    "    def retrieve_all_data_from_db(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Función que retorna la info de la base de datos para el schema y tablas del ejercicio\n",
    "        Nota: En caso de hacer consultas mas complejas esto debera refactorizarse pero por ahora va bien.\n",
    "\n",
    "        Returns\n",
    "        --------------------------------------------------------\n",
    "        data : dataframe.DataFrame -> Spark Dataframe\n",
    "        \"\"\"\n",
    "        query = f\"select * from {self._SCHEMA+self._table_name}\"\n",
    "        data = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", self._URL) \\\n",
    "            .option(\"dbtable\", f\"({query}) as data\") \\\n",
    "            .option(\"user\", self._USER) \\\n",
    "            .option(\"password\", self._PASSWORD) \\\n",
    "            .option(\"driver\", self._DRIVER) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def insert_data_into_db(self, dataframe: DataFrame) -> None:\n",
    "        \"\"\" \n",
    "        Función para insertar la info desde el dataframe del ejercicio. \n",
    "        Nota: Es valida puntualmente para este caso de uso. Pensar en escalarlo.\n",
    "\n",
    "        Params\n",
    "        ------------------------------------------------------\n",
    "        dataframe: dataframe.DataFrame -> Spark Dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe.write \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", self._URL) \\\n",
    "                .option(\"dbtable\", self._SCHEMA+self._table_name) \\\n",
    "                .option(\"user\", self._USER) \\\n",
    "                .option(\"password\", self._PASSWORD) \\\n",
    "                .option(\"driver\", self._DRIVER) \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save()\n",
    "            print(\"Información insertada correctamente\")\n",
    "        except: raise Exception(\"Ha ocurrido un error al insertar la data en el db\")\n",
    "\n",
    "    def close_connection(self) -> None:\n",
    "        self._conn.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e12da46",
   "metadata": {},
   "source": [
    "## Instanciamos la clase y realizamos las operaciones de insertar y leer desde la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b511d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl = ETL()\n",
    "\n",
    "try:\n",
    "    # Insertamos la información en el dataframe\n",
    "    # Debe verificarse que se imprima el mensaje de success y no suceda la excepción\n",
    "    etl.insert_data_into_db(dataframe=df)\n",
    "    # Asignamos a un nuevo dataframe la información obtenida desde la db\n",
    "    db_df = etl.retrieve_all_data_from_db()\n",
    "except:\n",
    "    print(\"Ocurrio un error\")\n",
    "finally:\n",
    "    # Debemos de tener en cuenta que una vez realizadas las operaciones deben cerrarse las conexiones.\n",
    "    etl.close_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df.printSchema()\n",
    "db_df.show()\n",
    "db_df.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
